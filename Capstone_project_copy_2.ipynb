{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\cvenkatanagasatya\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import RNN, GRU, LSTM, Dense, Input, Embedding, Dropout, Activation, concatenate\n",
    "from keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = []\n",
    "with open('dict.te-en.en.txt', 'r', encoding='utf-8') as f:\n",
    "    line = f.readlines()\n",
    "    for word in line:\n",
    "        english_data.append(word)\n",
    "        \n",
    "        \n",
    "telugu_data = []\n",
    "with open('dict.te-en.te.txt', 'r', encoding='utf-8') as f:\n",
    "    line = f.readlines()\n",
    "    for word in line:\n",
    "        telugu_data.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'english': english_data, 'telugu': telugu_data}\n",
    "data_frame = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.english = [re.sub(r'\\s','', i) for i in data_frame.english]\n",
    "data_frame.telugu = [re.sub(r'\\s','', i) for i in data_frame.telugu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "data_frame.english = data_frame.english.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "data_frame.telugu = data_frame.telugu.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.telugu = data_frame.telugu.apply(lambda x : 'START_ '+ x + ' _END')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>telugu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chitfundcircle</td>\n",
       "      <td>START_ చిట్‌ఫండుసర్కిలు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chitfundcircle</td>\n",
       "      <td>START_ చిట్‌ఫండుసర్కిలు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Poetically</td>\n",
       "      <td>START_ కవిత్వంచే _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kavitwamchey</td>\n",
       "      <td>START_ కవిత్వంచే _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poetry</td>\n",
       "      <td>START_ కవిత్వంచే _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          english                        telugu\n",
       "0  Chitfundcircle  START_ చిట్‌ఫండుసర్కిలు _END\n",
       "1  chitfundcircle  START_ చిట్‌ఫండుసర్కిలు _END\n",
       "2      Poetically         START_ కవిత్వంచే _END\n",
       "3    kavitwamchey         START_ కవిత్వంచే _END\n",
       "4          poetry         START_ కవిత్వంచే _END"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eng_words=set()\n",
    "for eng in data_frame.english:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "    \n",
    "all_tel_words=set()\n",
    "for tel in data_frame.telugu:\n",
    "    for word in tel.split():\n",
    "        if word not in all_tel_words:\n",
    "            all_tel_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26121, 12191)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words), len( all_tel_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_tel_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_tel_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26121, 12191)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_eng_length = []\n",
    "for word in data_frame.english:\n",
    "    #print(word)\n",
    "    max_eng_length.append(len(word.split(\" \")))\n",
    "    \n",
    "np.max(max_eng_length)\n",
    "\n",
    "\n",
    "max_tel_length = []\n",
    "for tel in data_frame.telugu:\n",
    "    #print(word)\n",
    "    max_tel_length.append(len(tel.split(\" \")))\n",
    "    \n",
    "np.max(max_tel_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(max_eng_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14497"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index['definitely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(data_frame.english)\n",
    "\n",
    "english_index = eng_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21068"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( english_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel_tokenizer = Tokenizer(filters='')\n",
    "tel_tokenizer.fit_on_texts(data_frame.telugu)\n",
    "\n",
    "telugu_index = tel_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12191"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(telugu_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21068, 12191)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_index), len(telugu_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'glove.6B/glove.6B.200d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('',embedding_file), encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#https://fasttext.cc/docs/en/english-vectors.html\n",
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "    \n",
    "    \n",
    "telugu_vectors = load_vectors('cc.te.300.vec')\n",
    "\n",
    "\n",
    "tel_num_words = len(all_tel_words)+1\n",
    "telugu_embedding_matrix = np.zeros((tel_num_words, 300))\n",
    "for word, i in telugu_index.items():\n",
    "    if i > tel_num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        telugu_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "telugu_embedding_layer = Embedding(\n",
    "    tel_num_words,\n",
    "    300,\n",
    "    weights=[telugu_embedding_matrix],\n",
    "    input_length = num_decoder_tokens)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26121"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_num_words = len(all_eng_words)+1\n",
    "english_embedding_matrix = np.zeros((eng_num_words, 200))\n",
    "for word, i in english_index.items():\n",
    "    if i > eng_num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        english_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26122"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embedding_layer = Embedding(\n",
    "    eng_num_words,\n",
    "    200,\n",
    "    weights=[english_embedding_matrix],\n",
    "    input_length = num_encoder_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(data_frame.english), 1),\n",
    "    dtype='float16')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(data_frame.telugu), 3),\n",
    "    dtype='float16')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(data_frame.telugu), 3, num_decoder_tokens),\n",
    "    dtype='float16')\n",
    "\n",
    "\n",
    "predict_encoder_data = np.zeros(\n",
    "    (1, 1),\n",
    "    dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(data_frame.english, data_frame.telugu)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38532, 1), (38532, 3, 12191))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape, decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 76.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.],\n",
       "       [123., 140., 340.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rough \n",
    "for i, (eng, tel) in enumerate(zip(lines.eng, lines.tel)):\n",
    "    for t, word in enumerate(eng.split()):\n",
    "        #print(t)\n",
    "        encoder_input_data[i,t] = w2i[word]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data[1,2] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 6., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x=  Embedding(num_encoder_tokens, 200)(encoder_inputs)\n",
    "encoder = LSTM(50, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dex=  Embedding(num_decoder_tokens, 200)\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 200)    5224200     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 200)    2438200     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 50), (None,  50200       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 50), ( 50200       embedding_4[0][0]                \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 12191)  621741      lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,384,541\n",
      "Trainable params: 8,384,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37376 samples, validate on 1156 samples\n",
      "Epoch 1/20\n",
      " 2368/37376 [>.............................] - ETA: 6:04 - loss: 4.8343 - acc: 0.3274"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[4,1] = 12192 is not in [0, 12191)\n\t [[Node: embedding_4/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_1/RMSprop/Assign_3\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_4/embeddings/read, embedding_4/Cast, training_1/RMSprop/gradients/embedding_4/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-fa6e6e0851e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[1;31m#batch_size=128,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           validation_split=0.03)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[4,1] = 12192 is not in [0, 12191)\n\t [[Node: embedding_4/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_1/RMSprop/Assign_3\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_4/embeddings/read, embedding_4/Cast, training_1/RMSprop/gradients/embedding_4/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_split=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivikramarao\n"
     ]
    }
   ],
   "source": [
    "for i, val in eng_tokenizer.word_index.items():\n",
    "    if val == 12190:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ఆయనపై\n"
     ]
    }
   ],
   "source": [
    "for i, val in tel_tokenizer.word_index.items():\n",
    "    if val == 12190:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12191, 21068)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tel_tokenizer.word_index), len(eng_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 383, 300)          115200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 50), (None, 50),  70200     \n",
      "=================================================================\n",
      "Total params: 185,400\n",
      "Trainable params: 185,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = encoder_input_data[1031: 1032]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2= dex(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'i dislike you'\n",
    "for t, word in enumerate(test.split()):\n",
    "    predict_encoder_data[0, t] = input_token_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' నువ్వు బాగోలేదని విన్నాను _END'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(predict_encoder_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: definitely!\n",
      "Decoded sentence:  తప్పకుండా _END\n",
      "-\n",
      "Input sentence: he hung up.\n",
      "Decoded sentence:  మాకు ఫ్రెంచి అర్ధం కాదు _END\n",
      "-\n",
      "Input sentence: i ran home.\n",
      "Decoded sentence:  నేను ఇంటికి పరిగెత్తాను _END\n",
      "-\n",
      "Input sentence: who are we?\n",
      "Decoded sentence:  మేము ఎవరము ? _END\n",
      "-\n",
      "Input sentence: are you mad?\n",
      "Decoded sentence:  కోపమొచ్చిందా ? _END\n",
      "-\n",
      "Input sentence: he touched me.\n",
      "Decoded sentence:  ఈ పెన్సిళ్లు ఒకే రంగులో _END\n",
      "-\n",
      "Input sentence: my head hurts.\n",
      "Decoded sentence:  నాకు గంట మోగటం వినపడింది _END\n",
      "-\n",
      "Input sentence: i drank coffee.\n",
      "Decoded sentence:  నేను కాఫీ తాగాను _END\n",
      "-\n",
      "Input sentence: how tall is she?\n",
      "Decoded sentence:  నువ్వు అది చూసావా ? _END\n",
      "-\n",
      "Input sentence: they're animals.\n",
      "Decoded sentence:  కోపమొచ్చిందా ? _END\n",
      "-\n",
      "Input sentence: can you see that?\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: i began to speak.\n",
      "Decoded sentence:  నేను మాట్లాడటం మొదలుపెట్టాను _END\n",
      "-\n",
      "Input sentence: i dislike coffee.\n",
      "Decoded sentence:  మేము వినాలని అనుకుంటున్నాం _END\n",
      "-\n",
      "Input sentence: i'm hungry again.\n",
      "Decoded sentence:  నాకు మళ్ళా ఆకలి వేస్తుంది _END\n",
      "-\n",
      "Input sentence: i don't accept it.\n",
      "Decoded sentence:  నేను అంగీకరించను _END\n",
      "-\n",
      "Input sentence: what's in the box?\n",
      "Decoded sentence:  మాకు ఫ్రెంచి అర్ధం కాదు _END\n",
      "-\n",
      "Input sentence: which is your pen?\n",
      "Decoded sentence:  నీ కలం ఏది ? _END\n",
      "-\n",
      "Input sentence: are you feeling ok?\n",
      "Decoded sentence:  అతను పెట్టేసాడు _END\n",
      "-\n",
      "Input sentence: help came too late.\n",
      "Decoded sentence:  అది ఇక్కడ చెయ్యడానికి _END\n",
      "-\n",
      "Input sentence: how are you coming?\n",
      "Decoded sentence:  నువ్వు ఎలా వస్తున్నావ్? _END\n",
      "-\n",
      "Input sentence: i do make mistakes.\n",
      "Decoded sentence:  నేనూ తప్పులు చేస్తాను _END\n",
      "-\n",
      "Input sentence: it wasn't my fault.\n",
      "Decoded sentence:  అది అంత తేలిక కాదు, తెలుసా _END\n",
      "-\n",
      "Input sentence: it's very valuable.\n",
      "Decoded sentence:  అది చాలా విలువైనది _END\n",
      "-\n",
      "Input sentence: she's not a doctor.\n",
      "Decoded sentence:  ఆమె వైద్యురాలు కాదు _END\n",
      "-\n",
      "Input sentence: we want to hear it.\n",
      "Decoded sentence:  మేము వినాలని అనుకుంటున్నాం _END\n",
      "-\n",
      "Input sentence: what will you have?\n",
      "Decoded sentence:  నువ్వు ఏమి తీసుకుంటావ్? _END\n",
      "-\n",
      "Input sentence: where was your son?\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: more coffee, please.\n",
      "Decoded sentence:  నాకు ఏమీ అవసరంలేదు. _END\n",
      "-\n",
      "Input sentence: tom was very scared.\n",
      "Decoded sentence:  అది చాలా విలువైనది _END\n",
      "-\n",
      "Input sentence: what are you saying?\n",
      "Decoded sentence:  నువ్వు ఎక్కడ ఉన్నవో అక్కడే ఉండు _END\n",
      "-\n",
      "Input sentence: you seem very happy.\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: i don't know who won.\n",
      "Decoded sentence:  నువ్వు చాల త్వరగా వచ్చావు _END\n",
      "-\n",
      "Input sentence: i'll be very careful.\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: i've thought of that.\n",
      "Decoded sentence:  నాకు అది తట్టింది _END\n",
      "-\n",
      "Input sentence: she sat on the bench.\n",
      "Decoded sentence:  ఆమె బల్ల మీద కూర్చుంది . _END\n",
      "-\n",
      "Input sentence: what else i can lose?\n",
      "Decoded sentence:  నువ్వు సిగ్గు పడాల్సిన అవసరం లేదు _END\n",
      "-\n",
      "Input sentence: i don't need anything.\n",
      "Decoded sentence:  నాకు ఏమీ అవసరంలేదు. _END\n",
      "-\n",
      "Input sentence: i heard the bell ring.\n",
      "Decoded sentence:  నాకు గంట మోగటం వినపడింది _END\n",
      "-\n",
      "Input sentence: i heard you were sick.\n",
      "Decoded sentence:  నీకు బాగోలేదని విన్నాను _END\n",
      "-\n",
      "Input sentence: i suggest you do that.\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: she's not at home now.\n",
      "Decoded sentence:  నేను ఎన్ని మామిడిపండ్లు కావాలి? _END\n",
      "-\n",
      "Input sentence: the station is nearby.\n",
      "Decoded sentence:  స్టేషన్ దగ్గర్లో వుంది _END\n",
      "-\n",
      "Input sentence: this made me very sad.\n",
      "Decoded sentence:  అది చాలా విలువైనది _END\n",
      "-\n",
      "Input sentence: i don't speak japanese.\n",
      "Decoded sentence:  నేను జపనీస్ మాట్లాడను _END\n",
      "-\n",
      "Input sentence: i don't trust that guy.\n",
      "Decoded sentence:  నేను వాడిని నమ్మను _END\n",
      "-\n",
      "Input sentence: let's take that chance.\n",
      "Decoded sentence:  మనం ఆ అవకాశం తీసుకుందాం _END\n",
      "-\n",
      "Input sentence: old people walk slowly.\n",
      "Decoded sentence:  ముసలివాళ్లు నెమ్మదిగా నడుస్తారు _END\n",
      "-\n",
      "Input sentence: she was pale with fear.\n",
      "Decoded sentence:  నాకు ఏమీ అవసరంలేదు. _END\n",
      "-\n",
      "Input sentence: there was nothing left.\n",
      "Decoded sentence:  ఇంకేమి మిగల్లేదు _END\n",
      "-\n",
      "Input sentence: this is not a sentence.\n",
      "Decoded sentence:  అది చాలా విలువైనది _END\n",
      "-\n",
      "Input sentence: was it really that bad?\n",
      "Decoded sentence:  అది అంత సులభం ఏం కాదు _END\n",
      "-\n",
      "Input sentence: can i make a phone call?\n",
      "Decoded sentence:  నువ్వు సిగ్గు పడాల్సిన అవసరం లేదు _END\n",
      "-\n",
      "Input sentence: he didn't keep his word.\n",
      "Decoded sentence:  అతను పిల్లలకి స్పానిష్ నేర్పుతున్నాడు _END\n",
      "-\n",
      "Input sentence: he made her a bookshelf.\n",
      "Decoded sentence:  అతడు స్త్రీలాగా వస్త్రాలను ధరించాడు. _END\n",
      "-\n",
      "Input sentence: it's not all your fault.\n",
      "Decoded sentence:  నీ కలం ఏది ? _END\n",
      "-\n",
      "Input sentence: she scared the cat away.\n",
      "Decoded sentence:  నువ్వు చాల త్వరగా వచ్చావు _END\n",
      "-\n",
      "Input sentence: the work is almost done.\n",
      "Decoded sentence:  చలనచిత్రం మొదలు అవ్వబోతుంది _END\n",
      "-\n",
      "Input sentence: what's your room number?\n",
      "Decoded sentence:  అది ఇక్కడ చెయ్యడానికి నాకు అనుమతి లేదు _END\n",
      "-\n",
      "Input sentence: he dressed up as a woman.\n",
      "Decoded sentence:  అతడు తన చేతులకి అంటిన రక్తాన్ని కడిగేసాడు _END\n",
      "-\n",
      "Input sentence: i'll let it go this time.\n",
      "Decoded sentence:  నేను ఇంటికి పరిగెత్తాను _END\n",
      "-\n",
      "Input sentence: none of them are present.\n",
      "Decoded sentence:  వాళ్లెవరు రాలేదు _END\n",
      "-\n",
      "Input sentence: we did that deliberately.\n",
      "Decoded sentence:  మేము అది కావాలని చేశాం _END\n",
      "-\n",
      "Input sentence: we haven't slept in days.\n",
      "Decoded sentence:  మేము రోజుల తరబడి నిద్రపోలేదు _END\n",
      "-\n",
      "Input sentence: you have to come at once.\n",
      "Decoded sentence:  నువ్వు ఒక్కసారిగా రావాలి _END\n",
      "-\n",
      "Input sentence: out of sight, out of mind.\n",
      "Decoded sentence:  చూపుకి వెలుపల​, మనసుకి వెలుపల​ _END\n",
      "-\n",
      "Input sentence: when did you quit smoking?\n",
      "Decoded sentence:  పొగ తాగడం ఎప్పుడు ఆపేశావ్? _END\n",
      "-\n",
      "Input sentence: where did you put my book?\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: you've arrived very early.\n",
      "Decoded sentence:  నువ్వు చాల త్వరగా వచ్చావు _END\n",
      "-\n",
      "Input sentence: brush your teeth every day.\n",
      "Decoded sentence:  నీ పళ్ళు రోజూ తోముకో _END\n",
      "-\n",
      "Input sentence: did all this really happen?\n",
      "Decoded sentence:  ఇది చెయ్యడానికి సిద్దంగా వున్నావా _END\n",
      "-\n",
      "Input sentence: some of the dogs are alive.\n",
      "Decoded sentence:  కొన్ని కుక్కలు బ్రతికే ఉన్నాయి _END\n",
      "-\n",
      "Input sentence: that wasn't easy, you know.\n",
      "Decoded sentence:  అది అంత తేలిక కాదు, తెలుసా _END\n",
      "-\n",
      "Input sentence: the movie's about to start.\n",
      "Decoded sentence:  చలనచిత్రం మొదలు అవ్వబోతుంది _END\n",
      "-\n",
      "Input sentence: we don't understand french.\n",
      "Decoded sentence:  మాకు ఫ్రెంచి అర్ధం కాదు _END\n",
      "-\n",
      "Input sentence: are you prepared to do this?\n",
      "Decoded sentence:  ఇది చెయ్యడానికి సిద్దంగా వున్నావా _END\n",
      "-\n",
      "Input sentence: i don't know where they are.\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: the baby is crying for milk.\n",
      "Decoded sentence:  పసిపాప పాల కోసం ఏడుస్తుంది _END\n",
      "-\n",
      "Input sentence: we'd better leave her alone.\n",
      "Decoded sentence:  నేను నువ్వైతే ఈరోజు అక్కడికి వెళ్ళను _END\n",
      "-\n",
      "Input sentence: you must stay where you are.\n",
      "Decoded sentence:  నువ్వు ఎక్కడ ఉన్నవో అక్కడే ఉండు _END\n",
      "-\n",
      "Input sentence: don't interfere with my work.\n",
      "Decoded sentence:  నాకు ఏమీ అవసరంలేదు. _END\n",
      "-\n",
      "Input sentence: how many mangoes do you want?\n",
      "Decoded sentence:  మీకు ఎన్ని మామిడిపండ్లు కావాలి? _END\n",
      "-\n",
      "Input sentence: she breathed in the cold air.\n",
      "Decoded sentence:  నువ్వు అది చూసావా ? _END\n",
      "-\n",
      "Input sentence: she was on her way to school.\n",
      "Decoded sentence:  నేను నువ్వైతే ఈరోజు అక్కడికి వెళ్ళను _END\n",
      "-\n",
      "Input sentence: we need money to do anything.\n",
      "Decoded sentence:  నా తల నొప్పిపుడుతుంది _END\n",
      "-\n",
      "Input sentence: i know you're going to say no.\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: it actually isn't that simple.\n",
      "Decoded sentence:  అది అంత సులభం ఏం కాదు _END\n",
      "-\n",
      "Input sentence: tell me about your daily life.\n",
      "Decoded sentence:  నీ రోజువారీ జీవితం గురించి చెప్పు _END\n",
      "-\n",
      "Input sentence: that sounds really interesting.\n",
      "Decoded sentence:  అది ఇక్కడ ఇంకా ఎక్కువ సమయం వుంచలేను _END\n",
      "-\n",
      "Input sentence: there's no reason to be afraid.\n",
      "Decoded sentence:  నువ్వు చాల త్వరగా వచ్చావు _END\n",
      "-\n",
      "Input sentence: we played basketball yesterday.\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: you have no need to be ashamed.\n",
      "Decoded sentence:  నువ్వు సిగ్గు పడాల్సిన అవసరం లేదు _END\n",
      "-\n",
      "Input sentence: you may go swimming or fishing.\n",
      "Decoded sentence:  నువ్వు ఈత కొట్టడానికో లేక చేపలు పట్టడానికో వెళ్ళొచ్చు\n",
      "-\n",
      "Input sentence: you must speak in a loud voice.\n",
      "Decoded sentence:  నువ్వు గట్టిగా మాట్లాడాలి _END\n",
      "-\n",
      "Input sentence: i'm not allowed to do that here.\n",
      "Decoded sentence:  అది ఇక్కడ ఇంకా ఎక్కువ సమయం వుంచలేను _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: my sister is crazy about tennis.\n",
      "Decoded sentence:  మా అక్కకి టెన్నిసంటే పిచ్చి _END\n",
      "-\n",
      "Input sentence: do you live in this neighborhood?\n",
      "Decoded sentence:  నువ్వు చాలా సంతోషంగా ఉన్నట్లున్నావ్ _END\n",
      "-\n",
      "Input sentence: have you ever had a heart attack?\n",
      "Decoded sentence:  నేను నువ్వైతే ఈరోజు అక్కడికి వెళ్ళను _END\n",
      "-\n",
      "Input sentence: i can't keep you here any longer.\n",
      "Decoded sentence:  అది ఇక్కడ ఇంకా ఎక్కువ సమయం వుంచలేను _END\n",
      "-\n",
      "Input sentence: she refuses to say more about it.\n",
      "Decoded sentence:  అది అంత సులభం ఏం కాదు _END\n",
      "-\n",
      "Input sentence: these pencils are the same color.\n",
      "Decoded sentence:  ఈ పెన్సిళ్లు ఒకే రంగులో ఉన్నాయి _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', lines.eng[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = 'Hello world'\n",
    "\n",
    "encoder_input_data[seq_index: seq_index + 1]\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input sentence:', lines.eng[seq_index])\n",
    "print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[143., 243., 138.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[2: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "134\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_input_text = []\n",
    "target_text = []\n",
    "\n",
    "for line in open('tel.txt', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    source_text = text[0]\n",
    "    translation = text[1]\n",
    "    \n",
    "    #target_translation = translation+'<eos>'\n",
    "    #target_translation_input = '<sos>'+translation\n",
    "    \n",
    "    input_text.append(source_text)\n",
    "    target_text.append(translation)\n",
    "    target_input_text.append(translation)\n",
    "    \n",
    "    \n",
    "output_input_text = target_input_text\n",
    "output_text = target_text \n",
    "    \n",
    "#target_input_text = target_input_text.insert('<sos>')\n",
    "#target_text = target_text + '<eos>'\n",
    "\n",
    "dummy = []\n",
    "for i in target_input_text:\n",
    "    dummy.append('<sos>'+str(i)+'<eos>')\n",
    "\n",
    "print(len(input_text))\n",
    "print(len(target_input_text))\n",
    "print(len(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input tokenizer\n",
    "tokenizer_input = Tokenizer()\n",
    "tokenizer_input.fit_on_texts(input_text)\n",
    "input_sequences = tokenizer_input.texts_to_sequences(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_index = tokenizer_input.word_index\n",
    "len(input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 1),\n",
       " ('i', 2),\n",
       " ('to', 3),\n",
       " ('the', 4),\n",
       " ('is', 5),\n",
       " ('that', 6),\n",
       " ('a', 7),\n",
       " ('are', 8),\n",
       " (\"don't\", 9),\n",
       " ('do', 10),\n",
       " ('she', 11),\n",
       " ('he', 12),\n",
       " ('we', 13),\n",
       " ('it', 14),\n",
       " ('was', 15),\n",
       " ('this', 16),\n",
       " ('me', 17),\n",
       " ('my', 18),\n",
       " ('can', 19),\n",
       " ('in', 20),\n",
       " ('your', 21),\n",
       " ('very', 22),\n",
       " ('have', 23),\n",
       " ('about', 24),\n",
       " ('how', 25),\n",
       " (\"i'm\", 26),\n",
       " ('not', 27),\n",
       " ('where', 28),\n",
       " ('know', 29),\n",
       " ('of', 30),\n",
       " ('out', 31),\n",
       " ('for', 32),\n",
       " (\"it's\", 33),\n",
       " ('need', 34),\n",
       " ('really', 35),\n",
       " ('her', 36),\n",
       " ('all', 37),\n",
       " ('did', 38),\n",
       " ('no', 39),\n",
       " ('who', 40),\n",
       " ('coffee', 41),\n",
       " ('speak', 42),\n",
       " ('want', 43),\n",
       " ('what', 44),\n",
       " ('more', 45),\n",
       " ('be', 46),\n",
       " ('anything', 47),\n",
       " ('at', 48),\n",
       " ('made', 49),\n",
       " ('with', 50),\n",
       " ('there', 51),\n",
       " ('as', 52),\n",
       " ('go', 53),\n",
       " ('time', 54),\n",
       " ('when', 55),\n",
       " ('going', 56),\n",
       " ('say', 57),\n",
       " ('here', 58),\n",
       " ('one', 59),\n",
       " ('eat', 60),\n",
       " ('up', 61),\n",
       " ('home', 62),\n",
       " (\"what's\", 63),\n",
       " ('which', 64),\n",
       " ('help', 65),\n",
       " ('came', 66),\n",
       " ('make', 67),\n",
       " (\"wasn't\", 68),\n",
       " ('fault', 69),\n",
       " (\"she's\", 70),\n",
       " ('doctor', 71),\n",
       " ('scared', 72),\n",
       " (\"i'll\", 73),\n",
       " (\"i've\", 74),\n",
       " ('on', 75),\n",
       " ('heard', 76),\n",
       " ('were', 77),\n",
       " ('now', 78),\n",
       " ('trust', 79),\n",
       " ('take', 80),\n",
       " ('people', 81),\n",
       " ('left', 82),\n",
       " ('bad', 83),\n",
       " (\"didn't\", 84),\n",
       " ('keep', 85),\n",
       " ('his', 86),\n",
       " ('cat', 87),\n",
       " ('work', 88),\n",
       " ('room', 89),\n",
       " ('them', 90),\n",
       " ('day', 91),\n",
       " ('happen', 92),\n",
       " ('understand', 93),\n",
       " ('must', 94),\n",
       " ('many', 95),\n",
       " ('way', 96),\n",
       " ('school', 97),\n",
       " (\"you're\", 98),\n",
       " (\"isn't\", 99),\n",
       " ('tell', 100),\n",
       " ('reason', 101),\n",
       " ('afraid', 102),\n",
       " ('or', 103),\n",
       " ('heart', 104),\n",
       " ('these', 105),\n",
       " ('pencils', 106),\n",
       " ('color', 107),\n",
       " ('off', 108),\n",
       " ('usually', 109),\n",
       " ('talk', 110),\n",
       " ('bus', 111),\n",
       " ('stop', 112),\n",
       " ('father', 113),\n",
       " ('and', 114),\n",
       " ('definitely', 115),\n",
       " ('hung', 116),\n",
       " ('ran', 117),\n",
       " ('mad', 118),\n",
       " ('touched', 119),\n",
       " ('head', 120),\n",
       " ('hurts', 121),\n",
       " ('drank', 122),\n",
       " ('tall', 123),\n",
       " (\"they're\", 124),\n",
       " ('animals', 125),\n",
       " ('see', 126),\n",
       " ('began', 127),\n",
       " ('dislike', 128),\n",
       " ('hungry', 129),\n",
       " ('again', 130),\n",
       " ('accept', 131),\n",
       " ('box', 132),\n",
       " ('pen', 133),\n",
       " ('feeling', 134),\n",
       " ('ok', 135),\n",
       " ('too', 136),\n",
       " ('late', 137),\n",
       " ('coming', 138),\n",
       " ('mistakes', 139),\n",
       " ('valuable', 140),\n",
       " ('hear', 141),\n",
       " ('will', 142),\n",
       " ('son', 143),\n",
       " ('please', 144),\n",
       " ('tom', 145),\n",
       " ('saying', 146),\n",
       " ('seem', 147),\n",
       " ('happy', 148),\n",
       " ('won', 149),\n",
       " ('careful', 150),\n",
       " ('thought', 151),\n",
       " ('sat', 152),\n",
       " ('bench', 153),\n",
       " ('else', 154),\n",
       " ('lose', 155),\n",
       " ('bell', 156),\n",
       " ('ring', 157),\n",
       " ('sick', 158),\n",
       " ('suggest', 159),\n",
       " ('station', 160),\n",
       " ('nearby', 161),\n",
       " ('sad', 162),\n",
       " ('japanese', 163),\n",
       " ('guy', 164),\n",
       " (\"let's\", 165),\n",
       " ('chance', 166),\n",
       " ('old', 167),\n",
       " ('walk', 168),\n",
       " ('slowly', 169),\n",
       " ('pale', 170),\n",
       " ('fear', 171),\n",
       " ('nothing', 172),\n",
       " ('sentence', 173),\n",
       " ('phone', 174),\n",
       " ('call', 175),\n",
       " ('word', 176),\n",
       " ('bookshelf', 177),\n",
       " ('away', 178),\n",
       " ('almost', 179),\n",
       " ('done', 180),\n",
       " ('number', 181),\n",
       " ('dressed', 182),\n",
       " ('woman', 183),\n",
       " ('let', 184),\n",
       " ('none', 185),\n",
       " ('present', 186),\n",
       " ('deliberately', 187),\n",
       " (\"haven't\", 188),\n",
       " ('slept', 189),\n",
       " ('days', 190),\n",
       " ('come', 191),\n",
       " ('once', 192),\n",
       " ('sight', 193),\n",
       " ('mind', 194),\n",
       " ('quit', 195),\n",
       " ('smoking', 196),\n",
       " ('put', 197),\n",
       " ('book', 198),\n",
       " (\"you've\", 199),\n",
       " ('arrived', 200),\n",
       " ('early', 201),\n",
       " ('brush', 202),\n",
       " ('teeth', 203),\n",
       " ('every', 204),\n",
       " ('some', 205),\n",
       " ('dogs', 206),\n",
       " ('alive', 207),\n",
       " ('easy', 208),\n",
       " (\"movie's\", 209),\n",
       " ('start', 210),\n",
       " ('french', 211),\n",
       " ('prepared', 212),\n",
       " ('they', 213),\n",
       " ('baby', 214),\n",
       " ('crying', 215),\n",
       " ('milk', 216),\n",
       " (\"we'd\", 217),\n",
       " ('better', 218),\n",
       " ('leave', 219),\n",
       " ('alone', 220),\n",
       " ('stay', 221),\n",
       " ('interfere', 222),\n",
       " ('mangoes', 223),\n",
       " ('breathed', 224),\n",
       " ('cold', 225),\n",
       " ('air', 226),\n",
       " ('money', 227),\n",
       " ('actually', 228),\n",
       " ('simple', 229),\n",
       " ('daily', 230),\n",
       " ('life', 231),\n",
       " ('sounds', 232),\n",
       " ('interesting', 233),\n",
       " (\"there's\", 234),\n",
       " ('played', 235),\n",
       " ('basketball', 236),\n",
       " ('yesterday', 237),\n",
       " ('ashamed', 238),\n",
       " ('may', 239),\n",
       " ('swimming', 240),\n",
       " ('fishing', 241),\n",
       " ('loud', 242),\n",
       " ('voice', 243),\n",
       " ('allowed', 244),\n",
       " ('sister', 245),\n",
       " ('crazy', 246),\n",
       " ('tennis', 247),\n",
       " ('live', 248),\n",
       " ('neighborhood', 249),\n",
       " ('ever', 250),\n",
       " ('had', 251),\n",
       " ('attack', 252),\n",
       " (\"can't\", 253),\n",
       " ('any', 254),\n",
       " ('longer', 255),\n",
       " ('refuses', 256),\n",
       " ('same', 257),\n",
       " ('pretty', 258),\n",
       " ('stupid', 259),\n",
       " ('question', 260),\n",
       " ('hesitate', 261),\n",
       " ('ask', 262),\n",
       " ('washed', 263),\n",
       " ('blood', 264),\n",
       " ('hands', 265),\n",
       " ('sorry', 266),\n",
       " ('last', 267),\n",
       " ('night', 268),\n",
       " ('does', 269),\n",
       " ('taught', 270),\n",
       " ('music', 271),\n",
       " ('thirty', 272),\n",
       " ('years', 273),\n",
       " ('from', 274),\n",
       " ('under', 275),\n",
       " ('desk', 276),\n",
       " ('tonight', 277),\n",
       " ('further', 278),\n",
       " ('right', 279),\n",
       " ('anymore', 280),\n",
       " ('sitting', 281),\n",
       " ('down', 282),\n",
       " ('hire', 283),\n",
       " ('lunch', 284),\n",
       " ('getting', 285),\n",
       " ('next', 286),\n",
       " ('teaching', 287),\n",
       " ('spanish', 288),\n",
       " ('children', 289),\n",
       " ('great', 290),\n",
       " ('poet', 291),\n",
       " ('well', 292),\n",
       " (\"wouldn't\", 293),\n",
       " ('today', 294),\n",
       " ('if', 295),\n",
       " ('asked', 296),\n",
       " ('languages', 297),\n",
       " ('spoke', 298),\n",
       " (\"you'll\", 299),\n",
       " ('prefer', 300),\n",
       " ('blue', 301),\n",
       " ('green', 302),\n",
       " ('breakfast', 303),\n",
       " ('before', 304),\n",
       " ('seven', 305),\n",
       " ('apparent', 306),\n",
       " ('why', 307),\n",
       " ('so', 308),\n",
       " ('angry', 309),\n",
       " (\"should've\", 310),\n",
       " ('been', 311),\n",
       " ('able', 312),\n",
       " ('by', 313),\n",
       " ('myself', 314),\n",
       " ('still', 315),\n",
       " ('something', 316),\n",
       " ('might', 317),\n",
       " ('high', 318),\n",
       " ('socks', 319),\n",
       " ('stretch', 320),\n",
       " ('wash', 321),\n",
       " ('pounding', 322),\n",
       " ('opened', 323),\n",
       " ('door', 324),\n",
       " ('nearest', 325),\n",
       " ('mistake', 326),\n",
       " ('though', 327),\n",
       " ('intend', 328),\n",
       " ('afternoon', 329),\n",
       " ('maybe', 330),\n",
       " ('often', 331),\n",
       " ('falls', 332),\n",
       " ('asleep', 333),\n",
       " ('while', 334),\n",
       " ('watching', 335),\n",
       " ('television', 336),\n",
       " ('has', 337),\n",
       " ('two', 338),\n",
       " ('long', 339),\n",
       " ('other', 340),\n",
       " ('short', 341)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "sorted(input_index.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_length = max(len(s) for s in input_sequences)\n",
    "max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output tokenizer\n",
    "tokenizer_output = Tokenizer(filters='')\n",
    "tokenizer_output.fit_on_texts(target_input_text + target_text)\n",
    "\n",
    "target_sequences = tokenizer_output.texts_to_sequences(target_text)\n",
    "target_input_sequences = tokenizer_output.texts_to_sequences(target_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_output_length = max(len(s) for s in target_sequences)\n",
    "max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_input_length = max(len(s) for s in target_input_sequences)\n",
    "target_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#telugu index \n",
    "telugu_tokenizer = Tokenizer()\n",
    "telugu_tokenizer.fit_on_texts(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_index = tokenizer_output.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "encoder_inputs = pad_sequences(input_sequences, max_input_length)\n",
    "decoder_inputs = pad_sequences(target_input_sequences, max_output_length, padding='post')\n",
    "decoder_output = pad_sequences(target_sequences, max_output_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 13), (134, 10), (134, 10))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs.shape, decoder_inputs.shape, decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'C:/Users/cvenkatanagasatya/Pictures/LazyProgrammer/machine_learning_examples/large_files/glove.6B/glove.6B.300d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('',embedding_file), encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (300) into shape (100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-51a660b3a283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (300) into shape (100)"
     ]
    }
   ],
   "source": [
    "num_words = len(input_index)+1\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in input_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    embed_size,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length = max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e2cbe447507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_input_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_text' is not defined"
     ]
    }
   ],
   "source": [
    "len(input_text), max_input_length, len(output_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_text),\n",
    "    max_input_length,\n",
    "    num_words+1\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 343 is out of bounds for axis 2 with size 343",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-76d133036d8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mdecoder_targets_one_hot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 343 is out of bounds for axis 2 with size 343"
     ]
    }
   ],
   "source": [
    "\n",
    "# assign the values\n",
    "for i, d in enumerate(decoder_output):\n",
    "    for t, word in enumerate(d):\n",
    "        if word != 0:\n",
    "            decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i,d in enumerate(decoder_output):\n",
    "   # print(i)\n",
    "    for t, word in enumerate(d):\n",
    "        test.append(word)\n",
    "        #decoder_target_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "encoder_input = Input(shape=(None, ))\n",
    "embed = embedding_layer(encoder_input)\n",
    "encoder = LSTM(100, return_state = True)\n",
    "encoder_output, h, c = encoder(embed)\n",
    "\n",
    "encoder_states = [h,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, ))\n",
    "embed_decoder = embedding_layer(decoder_input)\n",
    "decoder_lstm = LSTM(100, return_state=True)\n",
    "decoder_target, _, _ = decoder_lstm(embed_decoder, initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(max_output_length, activation='softmax')\n",
    "decoder_target = decoder_dense(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_input, decoder_input], decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 13, 100)      34200       input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 100), (None, 80400       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 100), (None, 80400       embedding_3[1][0]                \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1010        lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 196,010\n",
      "Trainable params: 196,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107 samples, validate on 27 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[13,3] = 342 is not in [0, 342)\n\t [[Node: embedding_3_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3_1/Cast, embedding_3/embedding_lookup/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-97a140e68039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           validation_split=0.2)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[0;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                                              verbose=0)\n\u001b[0m\u001b[0;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                         \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowproject\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[13,3] = 342 is not in [0, 342)\n\t [[Node: embedding_3_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3_1/Cast, embedding_3/embedding_lookup/axis)]]"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_inputs, decoder_inputs], decoder_output,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflowproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
